# Controle Financeiro API

Este √© o reposit√≥rio do back-end do projeto **Controle Financeiro**, desenvolvido como parte do MVP (Minimum Viable Product) para a P√≥s-Gradua√ß√£o em Engenharia de Software da PUC Rio. O objetivo do projeto √© fornecer uma plataforma inteligente para gerenciar finan√ßas pessoais, permitindo o controle de transa√ß√µes, categorias e usu√°rios de forma eficiente, com **classifica√ß√£o autom√°tica** de transa√ß√µes utilizando **Machine Learning**.

Outros reposit√≥rios associados:
* Front-end [https://github.com/GuilhermePFM/mvp-front-end](https://github.com/GuilhermePFM/mvp-front-end).
* Embedding API [https://github.com/GuilhermePFM/mvp-front-end](https://github.com/GuilhermePFM/mvp-embedding).
---

## √çndice
- [Controle Financeiro API](#controle-financeiro-api)
  - [√çndice](#√≠ndice)
  - [Descri√ß√£o](#descri√ß√£o)
  - [Arquitetura do Sistema](#arquitetura-do-sistema)
  - [Funcionalidades Principais](#funcionalidades-principais)
    - [üóÑÔ∏è Gerenciamento de Dados (CRUD)](#Ô∏è-gerenciamento-de-dados-crud)
    - [ü§ñ Classifica√ß√£o Inteligente com Machine Learning](#-classifica√ß√£o-inteligente-com-machine-learning)
    - [üîí Seguran√ßa](#-seguran√ßa)
    - [üìä Documenta√ß√£o Autom√°tica](#-documenta√ß√£o-autom√°tica)
  - [Estrutura de Arquivos do Projeto](#estrutura-de-arquivos-do-projeto)
    - [Descri√ß√£o dos Diret√≥rios](#descri√ß√£o-dos-diret√≥rios)
      - [**`apis/`**](#apis)
      - [**`machine_learning/`**](#machine_learning)
      - [**`model/`**](#model)
      - [**`schemas/`**](#schemas)
      - [**`security/`**](#security)
      - [**`tests/`**](#tests)
  - [Como Funciona a Classifica√ß√£o Inteligente](#como-funciona-a-classifica√ß√£o-inteligente)
    - [Fluxo de Classifica√ß√£o](#fluxo-de-classifica√ß√£o)
    - [Tecnologias de Machine Learning](#tecnologias-de-machine-learning)
  - [Tecnologias Utilizadas](#tecnologias-utilizadas)
    - [Backend e API](#backend-e-api)
    - [Banco de Dados](#banco-de-dados)
    - [Machine Learning](#machine-learning)
    - [Seguran√ßa](#seguran√ßa)
    - [Desenvolvimento e Testes](#desenvolvimento-e-testes)
  - [Configura√ß√£o do Ambiente](#configura√ß√£o-do-ambiente)
    - [Vari√°veis de Ambiente](#vari√°veis-de-ambiente)
  - [Como Instalar as Depend√™ncias](#como-instalar-as-depend√™ncias)
    - [1. **Clone o Reposit√≥rio**](#1-clone-o-reposit√≥rio)
    - [2. **Crie um Ambiente Virtual**](#2-crie-um-ambiente-virtual)
    - [3. **Ative o Ambiente Virtual**](#3-ative-o-ambiente-virtual)
    - [4. **Instale as Depend√™ncias**](#4-instale-as-depend√™ncias)
    - [5. **Configure as Vari√°veis de Ambiente**](#5-configure-as-vari√°veis-de-ambiente)
  - [Como Executar o C√≥digo](#como-executar-o-c√≥digo)
    - [1. **Inicie a Aplica√ß√£o**](#1-inicie-a-aplica√ß√£o)
    - [2. **Acesse a API**](#2-acesse-a-api)
    - [3. **Acesse a Documenta√ß√£o Interativa**](#3-acesse-a-documenta√ß√£o-interativa)
  - [üê≥ Implanta√ß√£o com Docker](#-implanta√ß√£o-com-docker)
    - [Pr√©-requisitos](#pr√©-requisitos)
    - [Desenvolvimento Local com Docker](#desenvolvimento-local-com-docker)
      - [1. **Configure as Vari√°veis de Ambiente**](#1-configure-as-vari√°veis-de-ambiente)
      - [2. **Build e Inicie os Containers**](#2-build-e-inicie-os-containers)
      - [3. **Verifique os Logs**](#3-verifique-os-logs)
      - [4. **Acesse a Aplica√ß√£o**](#4-acesse-a-aplica√ß√£o)
      - [5. **Parar a Aplica√ß√£o**](#5-parar-a-aplica√ß√£o)
    - [Volumes e Persist√™ncia de Dados](#volumes-e-persist√™ncia-de-dados)
    - [Portas Expostas](#portas-expostas)
  - [Endpoints da API](#endpoints-da-api)
    - [Documenta√ß√£o Interativa](#documenta√ß√£o-interativa)
    - [Principais Endpoints](#principais-endpoints)
      - [ü§ñ **Classifica√ß√£o de Transa√ß√µes**](#-classifica√ß√£o-de-transa√ß√µes)
      - [üë§ **Usu√°rios**](#-usu√°rios)
      - [üí∞ **Transa√ß√µes**](#-transa√ß√µes)
      - [üè∑Ô∏è **Categorias**](#Ô∏è-categorias)
      - [üìã **Tipos de Transa√ß√£o**](#-tipos-de-transa√ß√£o)
  - [Testes](#testes)
    - [Executar Todos os Testes](#executar-todos-os-testes)
    - [Executar Testes Espec√≠ficos](#executar-testes-espec√≠ficos)
    - [Executar com Cobertura](#executar-com-cobertura)
    - [Estrutura de Testes](#estrutura-de-testes)
  - [Seguran√ßa](#seguran√ßa-1)
    - [üîê Criptografia de Dados](#-criptografia-de-dados)
    - [üõ°Ô∏è Prote√ß√£o de Informa√ß√µes Pessoais](#Ô∏è-prote√ß√£o-de-informa√ß√µes-pessoais)

---

## Descri√ß√£o

A **API ControleFinanceiro** √© um sistema de gerenciamento financeiro desenvolvido para ajudar os usu√°rios a rastrear suas transa√ß√µes, gerenciar categorias e organizar suas finan√ßas de forma inteligente. Constru√≠da com **Python** e **Flask**, a API utiliza **SQLAlchemy** para gerenciamento de banco de dados, **Pydantic** para valida√ß√£o de esquemas, e **scikit-learn** para classifica√ß√£o autom√°tica de transa√ß√µes.

O diferencial deste sistema √© a **classifica√ß√£o autom√°tica de transa√ß√µes** usando modelos de Machine Learning treinados, que analisam a descri√ß√£o, valor e data das transa√ß√µes para categoriz√°-las automaticamente, economizando tempo e aumentando a precis√£o no controle financeiro.

---

## Arquitetura do Sistema

O projeto **Controle Financeiro** √© composto por **tr√™s microservi√ßos** que trabalham em conjunto:

![alt text](backend.png)

**1. Interface (Front-end):** Aplica√ß√£o web onde o usu√°rio interage, faz upload de transa√ß√µes e visualiza os resultados.

**2. Backend API (Este Reposit√≥rio):** Servi√ßo principal que gerencia dados, executa classifica√ß√µes e coordena a comunica√ß√£o entre os servi√ßos.

**3. API de Embeddings:** Servi√ßo externo (Google Gemini API) que gera embeddings sem√¢nticos das descri√ß√µes das transa√ß√µes para melhorar a precis√£o da classifica√ß√£o.



---

## Funcionalidades Principais

### üóÑÔ∏è Gerenciamento de Dados (CRUD)
- **Usu√°rios**: Criar, listar, buscar e excluir usu√°rios
- **Transa√ß√µes**: Adicionar transa√ß√µes individuais ou em lote, listar e excluir
- **Categorias**: Gerenciar categorias de transa√ß√µes (ex: Alimenta√ß√£o, Transporte, Lazer)
- **Tipos de Transa√ß√£o**: Gerenciar tipos (Receita, Despesa)

### ü§ñ Classifica√ß√£o Inteligente com Machine Learning
- **Classifica√ß√£o em Lote**: Endpoint `/batchclassifier` que classifica m√∫ltiplas transa√ß√µes automaticamente
- **Modelo Treinado**: Utiliza modelo scikit-learn treinado com dados hist√≥ricos
- **Feature Engineering Avan√ßada**: 
  - Embeddings sem√¢nticos das descri√ß√µes via Google Gemini API
  - Extra√ß√£o de palavras-chave relevantes (PIX, Uber, etc.)
  - Features temporais (dia, m√™s)
  - Normaliza√ß√£o de valores num√©ricos

### üîí Seguran√ßa
- **Criptografia de Dados**: Datasets sens√≠veis s√£o criptografados usando chaves sim√©tricas
- **Prote√ß√£o de Informa√ß√µes**: Sistema de criptografia para proteger dados financeiros armazenados

### üìä Documenta√ß√£o Autom√°tica
- **Swagger UI**: Interface interativa para testar todos os endpoints
- **OpenAPI 3.0**: Documenta√ß√£o completa e autom√°tica da API

---

## Estrutura de Arquivos do Projeto

```
ControleFinanceiro/
‚îú‚îÄ‚îÄ apis/                          # Endpoints da API
‚îÇ   ‚îú‚îÄ‚îÄ batch_classifier.py        # Endpoint de classifica√ß√£o em lote
‚îÇ   ‚îú‚îÄ‚îÄ transactions.py            # CRUD de transa√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ transactions_category.py   # CRUD de categorias
‚îÇ   ‚îú‚îÄ‚îÄ transactions_type.py       # CRUD de tipos
‚îÇ   ‚îî‚îÄ‚îÄ users.py                   # CRUD de usu√°rios
‚îÇ
‚îú‚îÄ‚îÄ machine_learning/              # M√≥dulo de Machine Learning
‚îÇ   ‚îú‚îÄ‚îÄ transactions_classification/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/                   # Bibliotecas de processamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py     # Carregamento de dados
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py  # Extra√ß√£o de features
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_embedding.py     # Integra√ß√£o com Gemini API
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py      # Preprocessamento de dados
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenization.py    # Tokeniza√ß√£o de texto
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training.py        # Treinamento de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                # Modelos treinados (.pkl)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification_model.pkl
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_classification_model.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/             # Pipelines de preprocessamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classification_preprocessor.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/                  # Datasets para treinamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ notebooks/             # Notebooks de experimenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ transactions_classifier.py # Classe principal do classificador
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Utilit√°rios gerais
‚îÇ
‚îú‚îÄ‚îÄ model/                         # Modelos de dados (SQLAlchemy)
‚îÇ   ‚îú‚îÄ‚îÄ base.py                    # Classe base para modelos
‚îÇ   ‚îú‚îÄ‚îÄ batch_job.py               # Modelo de job ass√≠ncrono (NOVO)
‚îÇ   ‚îú‚îÄ‚îÄ transaction.py             # Modelo de transa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ transaction_category.py    # Modelo de categoria
‚îÇ   ‚îú‚îÄ‚îÄ transaction_type.py        # Modelo de tipo
‚îÇ   ‚îî‚îÄ‚îÄ user.py                    # Modelo de usu√°rio
‚îÇ
‚îú‚îÄ‚îÄ schemas/                       # Schemas de valida√ß√£o (Pydantic)
‚îÇ   ‚îú‚îÄ‚îÄ batch_classifier.py        # Schema para classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ batch_job.py               # Schema para jobs ass√≠ncronos (NOVO)
‚îÇ   ‚îú‚îÄ‚îÄ error.py                   # Schema de erros
‚îÇ   ‚îú‚îÄ‚îÄ transaction.py             # Schema de transa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ transaction_category.py    # Schema de categoria
‚îÇ   ‚îú‚îÄ‚îÄ transaction_type.py        # Schema de tipo
‚îÇ   ‚îî‚îÄ‚îÄ user.py                    # Schema de usu√°rio
‚îÇ
‚îú‚îÄ‚îÄ security/                      # M√≥dulos de seguran√ßa
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                 # Criptografia de datasets
‚îÇ   ‚îú‚îÄ‚îÄ salting.py                 # Gera√ß√£o de salts
‚îÇ   ‚îú‚îÄ‚îÄ signing.py                 # Assinatura digital
‚îÇ   ‚îú‚îÄ‚îÄ user_data.py               # Prote√ß√£o de dados do usu√°rio
‚îÇ   ‚îî‚îÄ‚îÄ symmetric/                 # Criptografia sim√©trica
‚îÇ       ‚îî‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ tests/                         # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ test_async_batch_classifier.py  # Testes de endpoints ass√≠ncronos (NOVO)
‚îÇ   ‚îú‚îÄ‚îÄ test_dataset_encryption.py # Testes de criptografia
‚îÇ   ‚îú‚îÄ‚îÄ test_integration_transactions_ml_model.py  # Testes de integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_transactions_ml_model.py  # Testes do modelo ML
‚îÇ   ‚îî‚îÄ‚îÄ classification_fixtures/   # Fixtures para testes
‚îÇ
‚îú‚îÄ‚îÄ database/                      # Banco de dados SQLite
‚îÇ   ‚îî‚îÄ‚îÄ db.sqlite3
‚îÇ
‚îú‚îÄ‚îÄ kafka/                        # Infraestrutura Kafka para processamento ass√≠ncrono
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                # Package init
‚îÇ   ‚îú‚îÄ‚îÄ batch_job_publisher.py    # Publisher para jobs ass√≠ncronos
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_worker.py      # Worker 1: Processa embeddings
‚îÇ   ‚îú‚îÄ‚îÄ classification_worker.py  # Worker 2: Executa classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                   # Utilit√°rios (retry, backoff)
‚îÇ   ‚îú‚îÄ‚îÄ job_cleanup.py             # Script de limpeza de jobs antigos
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_consumer.py     # Consumer legado de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ text_description_publisher.py  # Publisher legado
‚îÇ   ‚îî‚îÄ‚îÄ process_classification_data.py # Processamento legado
‚îÇ
‚îú‚îÄ‚îÄ template/                      # Template Excel para upload
‚îÇ   ‚îî‚îÄ‚îÄ template.xlsx
‚îÇ
‚îú‚îÄ‚îÄ log/                           # Logs da aplica√ß√£o
‚îÇ
‚îú‚îÄ‚îÄ app.py                         # Configura√ß√£o principal do Flask
‚îú‚îÄ‚îÄ config.py                      # Configura√ß√£o do OpenAPI/Swagger
‚îú‚îÄ‚îÄ logger.py                      # Sistema de logs
‚îú‚îÄ‚îÄ requirements.txt               # Depend√™ncias Python
‚îú‚îÄ‚îÄ env.example                    # Exemplo de vari√°veis de ambiente
‚îî‚îÄ‚îÄ README.md                      # Este arquivo
```

### Descri√ß√£o dos Diret√≥rios

#### **`apis/`**
Cont√©m todos os endpoints REST da API organizados por dom√≠nio:
- **`batch_classifier.py`**: Endpoint para classifica√ß√£o autom√°tica em lote de transa√ß√µes usando ML
- **`transactions.py`**: Endpoints para gerenciar transa√ß√µes (adicionar, listar, excluir)
- **`transactions_category.py`**: Endpoints para gerenciar categorias de transa√ß√µes
- **`transactions_type.py`**: Endpoints para gerenciar tipos de transa√ß√µes
- **`users.py`**: Endpoints para gerenciar usu√°rios do sistema

#### **`machine_learning/`**
M√≥dulo completo de Machine Learning para classifica√ß√£o de transa√ß√µes:
- **`lib/`**: Bibliotecas customizadas para processamento e treinamento
  - Feature engineering (extra√ß√£o de palavras-chave, datas)
  - Integra√ß√£o com Google Gemini API para embeddings
  - Preprocessamento e normaliza√ß√£o de dados
  - Tokeniza√ß√£o de texto
- **`models/`**: Modelos treinados salvos em formato pickle
- **`pipelines/`**: Pipelines de preprocessamento serializados
- **`notebooks/`**: Jupyter notebooks para experimenta√ß√£o e an√°lise
- **`transactions_classifier.py`**: Classe principal que encapsula o modelo de classifica√ß√£o

#### **`model/`**
Define os modelos de dados usando SQLAlchemy ORM:
- **`base.py`**: Classe base para todos os modelos
- **`transaction.py`**: Modelo de transa√ß√£o financeira
- **`transaction_category.py`**: Modelo de categoria (ex: Alimenta√ß√£o, Transporte)
- **`transaction_type.py`**: Modelo de tipo (Receita ou Despesa)
- **`user.py`**: Modelo de usu√°rio

#### **`schemas/`**
Define os schemas de valida√ß√£o usando Pydantic:
- Valida√ß√£o autom√°tica de dados de entrada
- Serializa√ß√£o de respostas da API
- Documenta√ß√£o autom√°tica no Swagger

#### **`security/`**
M√≥dulos de seguran√ßa para prote√ß√£o de dados sens√≠veis:
- Criptografia sim√©trica de datasets
- Prote√ß√£o de informa√ß√µes financeiras
- Assinatura e valida√ß√£o de dados

#### **`tests/`**
Su√≠te completa de testes automatizados:
- Testes de integra√ß√£o do modelo ML
- Testes de criptografia
- Fixtures para valida√ß√£o de performance do modelo

---

## Como Funciona a Classifica√ß√£o Inteligente

### Fluxo de Classifica√ß√£o

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FLUXO DE CLASSIFICA√á√ÉO DE TRANSA√á√ïES                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. üì§ Usu√°rio envia transa√ß√µes
   ‚îî‚îÄ‚ñ∫ Endpoint: POST /batchclassifier
       Payload: [{value, date, description}, ...]

2. üîÑ Preprocessamento
   ‚îî‚îÄ‚ñ∫ Normaliza√ß√£o de valores
   ‚îî‚îÄ‚ñ∫ Parsing de datas
   ‚îî‚îÄ‚ñ∫ Limpeza de descri√ß√µes

3. üß¨ Feature Engineering
   ‚îî‚îÄ‚ñ∫ Extra√ß√£o de palavras-chave (pix, uber, ifd, pag, etc.)
   ‚îî‚îÄ‚ñ∫ Features temporais (dia, m√™s)
   ‚îî‚îÄ‚ñ∫ Normaliza√ß√£o de valores num√©ricos
   ‚îî‚îÄ‚ñ∫ Embeddings sem√¢nticos via Google Gemini API

4. ü§ñ Classifica√ß√£o
   ‚îî‚îÄ‚ñ∫ Pipeline de preprocessamento carregado
   ‚îî‚îÄ‚ñ∫ Modelo scikit-learn faz predi√ß√µes
   ‚îî‚îÄ‚ñ∫ Cada transa√ß√£o recebe uma categoria

5. üì• Retorno dos Resultados
   ‚îî‚îÄ‚ñ∫ Transa√ß√µes com categorias atribu√≠das
   ‚îî‚îÄ‚ñ∫ Formato JSON estruturado
```

### Tecnologias de Machine Learning

**Modelo de Classifica√ß√£o:**
- **Framework**: scikit-learn
- **Tipo**: Classifica√ß√£o supervisionada
- **Features**:
  - **Num√©ricas**: Valores das transa√ß√µes (normalizados)
  - **Temporais**: Dia e m√™s da transa√ß√£o
  - **Textuais**: Embeddings das descri√ß√µes + keywords extra√≠das
  
**Embeddings Sem√¢nticos:**
- **Provedor**: Google Gemini API
- **Modelo**: `gemini-embedding-001`
- **Dimensionalidade**: 768 dimens√µes
- **Normaliza√ß√£o**: Embeddings normalizados para melhor performance

**Feature Engineering:**
- Detec√ß√£o de palavras-chave relevantes: `pix`, `uber`, `ifd`, `pag`, `aplica√ß√£o`, `sal√°rio`, `light`
- Extra√ß√£o de features temporais: dia e m√™s da transa√ß√£o
- Normaliza√ß√£o de valores num√©ricos usando `StandardScaler`
- Normaliza√ß√£o de embeddings usando `Normalizer`

**Pipeline de Preprocessamento:**
- `ColumnTransformer` para aplicar transforma√ß√µes espec√≠ficas por tipo de feature
- `FeatureUnion` para combinar diferentes representa√ß√µes textuais
- Serializa√ß√£o completa para garantir consist√™ncia em produ√ß√£o

---

## Arquitetura Ass√≠ncrona de Classifica√ß√£o em Lote

Para lidar com volumes maiores de dados e tempos de resposta mais longos do servi√ßo de embeddings, o sistema oferece uma **arquitetura ass√≠ncrona baseada em Kafka** que desacopla o processamento de classifica√ß√£o da requisi√ß√£o HTTP.

### Fluxo Ass√≠ncrono

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ARQUITETURA ASS√çNCRONA DE CLASSIFICA√á√ÉO                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. üì§ Frontend: POST /api/batch-classify-async
   ‚îî‚îÄ‚ñ∫ Retorna imediatamente: { "jobId": "uuid" }
   ‚îî‚îÄ‚ñ∫ Status: 202 Accepted

2. üíæ Job criado no banco de dados
   ‚îî‚îÄ‚ñ∫ Status: pending
   ‚îî‚îÄ‚ñ∫ Dados armazenados: transa√ß√µes de entrada

3. üì® Job publicado no Kafka
   ‚îî‚îÄ‚ñ∫ T√≥pico: batch-jobs
   ‚îî‚îÄ‚ñ∫ Workers consomem assincronamente

4. üîÑ Worker 1: Embeddings
   ‚îú‚îÄ‚ñ∫ Consome de: batch-jobs
   ‚îú‚îÄ‚ñ∫ Atualiza status: processing
   ‚îú‚îÄ‚ñ∫ Chama API externa de embeddings
   ‚îú‚îÄ‚ñ∫ Retry autom√°tico (3x com backoff exponencial)
   ‚îî‚îÄ‚ñ∫ Publica em: embeddings-results

5. ü§ñ Worker 2: Classification
   ‚îú‚îÄ‚ñ∫ Consome de: embeddings-results
   ‚îú‚îÄ‚ñ∫ Combina embeddings + transa√ß√µes
   ‚îú‚îÄ‚ñ∫ Executa modelo ML de classifica√ß√£o
   ‚îú‚îÄ‚ñ∫ Atualiza status: completed
   ‚îî‚îÄ‚ñ∫ Armazena resultados no banco

6. üîç Frontend: Polling GET /api/batch-jobs/{jobId}
   ‚îú‚îÄ‚ñ∫ Status: pending ‚Üí processing ‚Üí completed
   ‚îú‚îÄ‚ñ∫ Quando completed: retorna transa√ß√µes classificadas
   ‚îî‚îÄ‚ñ∫ Job √© deletado ap√≥s fetch bem-sucedido
```

### Componentes da Arquitetura

**1. API Endpoints**

**`POST /api/batch-classify-async`**
- Aceita lista de transa√ß√µes
- Cria job no banco de dados
- Publica no Kafka
- Retorna `202 Accepted` com `jobId`

**`GET /api/batch-jobs/{jobId}`**
- Retorna status atual do job
- Estados poss√≠veis: `pending`, `processing`, `completed`, `failed`
- Quando `completed`: inclui transa√ß√µes classificadas
- Job √© deletado ap√≥s fetch bem-sucedido

**2. T√≥picos Kafka**

| T√≥pico | Produtor | Consumidor | Conte√∫do |
|--------|----------|------------|----------|
| `batch-jobs` | API Flask | Embeddings Worker | Job ID + transa√ß√µes |
| `embeddings-results` | Embeddings Worker | Classification Worker | Job ID + transa√ß√µes + embeddings |

**3. Workers**

**Embeddings Worker** (`kafka/embeddings_worker.py`)
- Consome jobs do t√≥pico `batch-jobs`
- Extrai descri√ß√µes das transa√ß√µes
- Chama API externa de embeddings com retry (3x)
- Publica resultados em `embeddings-results`
- Gerencia falhas e atualiza status no banco

**Classification Worker** (`kafka/classification_worker.py`)
- Consome do t√≥pico `embeddings-results`
- Combina embeddings com dados das transa√ß√µes
- Executa modelo ML de classifica√ß√£o
- Salva resultados no banco de dados
- Marca job como `completed`

**4. Banco de Dados**

**Modelo `BatchJob`** (`model/batch_job.py`)
- `id`: UUID √∫nico do job
- `status`: pending | processing | completed | failed
- `created_at`, `updated_at`: Timestamps
- `transactions_input`: JSON com transa√ß√µes de entrada
- `transactions_output`: JSON com transa√ß√µes classificadas
- `error_message`: Mensagem de erro (se falhar)
- `retry_count`: Contador de tentativas

### Tratamento de Erros e Retry

**Estrat√©gia de Retry**
- **3 tentativas** para chamadas √† API de embeddings
- **Backoff exponencial**: 2s ‚Üí 4s ‚Üí 8s
- Erros s√£o logados com detalhes completos
- Job marcado como `failed` ap√≥s esgotamento de tentativas

**Estados de Erro**
- `failed`: Erro irrecuper√°vel (API indispon√≠vel, dados inv√°lidos)
- Mensagem de erro detalhada retornada no GET

**Cleanup Autom√°tico**
- Jobs completados s√£o deletados ap√≥s fetch
- Fallback: Script `kafka/job_cleanup.py` remove jobs com > 24h
- Pode ser executado via cron para manuten√ß√£o

### Como Executar o Sistema Ass√≠ncrono

**1. Iniciar Kafka**
```bash
# Usando Docker (recomendado)
docker run -d --name kafka \
  -p 9092:9092 \
  -e KAFKA_ENABLE_KRAFT=yes \
  apache/kafka:latest

# Ou instale localmente e inicie o broker
```

**2. Configurar Vari√°veis de Ambiente**
```bash
# Adicione ao arquivo .env
KAFKA_BROKER_ADDRESS=localhost:9092
BATCH_JOBS_TOPIC=batch-jobs
EMBEDDINGS_RESULTS_TOPIC=embeddings-results
EMBEDDINGS_CONSUMER_GROUP=embeddings_worker
CLASSIFICATION_CONSUMER_GROUP=classification_worker
EMBEDDING_API_URL=http://localhost:8000
```

**3. Iniciar Workers**
```bash
# Terminal 1: Embeddings Worker
python kafka/embeddings_worker.py

# Terminal 2: Classification Worker
python kafka/classification_worker.py
```

**4. Iniciar API**
```bash
# Terminal 3: Flask API
python app.py
```

**5. Manuten√ß√£o (Opcional)**
```bash
# Executar cleanup manual de jobs antigos
python kafka/job_cleanup.py

# Ou agendar via cron (Linux/Mac)
# Adicione ao crontab: 0 2 * * * cd /path/to/project && python kafka/job_cleanup.py
```

### Exemplo de Uso

**Requisi√ß√£o:**
```bash
curl -X POST http://localhost:5000/api/batch-classify-async \
  -H "Content-Type: application/json" \
  -d '{
    "transactions": [
      {
        "date": "2024-01-15T00:00:00",
        "description": "Grocery shopping",
        "value": 150.50,
        "user": "John Doe",
        "classification": null
      }
    ]
  }'
```

**Resposta (202 Accepted):**
```json
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Polling Status:**
```bash
# Polling a cada 2 segundos
curl http://localhost:5000/api/batch-jobs/550e8400-e29b-41d4-a716-446655440000

# Resposta (processing):
{"status": "processing"}

# Resposta (completed):
{
  "status": "completed",
  "transactions": [
    {
      "date": "2024-01-15T00:00:00",
      "description": "Grocery shopping",
      "value": 150.50,
      "user": "John Doe",
      "classification": "Food & Groceries"
    }
  ]
}
```

### Monitoramento e Troubleshooting

**Verificar Status dos Workers**
```bash
# Workers devem exibir logs como:
# ============================================================
# EMBEDDINGS WORKER STARTING
# ============================================================
# Starting embeddings worker, consuming from batch-jobs
```

**Logs Importantes**
- Workers logam cada mensagem processada
- Erros s√£o logados com stack trace completo
- Kafka offsets s√£o commitados apenas ap√≥s sucesso

**Problemas Comuns**

| Problema | Causa Prov√°vel | Solu√ß√£o |
|----------|----------------|---------|
| Job fica `pending` indefinidamente | Workers n√£o est√£o rodando | Inicie os workers |
| Status `failed` com erro de API | API de embeddings offline | Verifique `EMBEDDING_API_URL` |
| Workers crasham ao iniciar | Kafka n√£o est√° acess√≠vel | Verifique `KAFKA_BROKER_ADDRESS` |
| Job n√£o encontrado (404) | Job j√° foi fetcheado | Jobs s√£o deletados ap√≥s fetch |

### Performance e Escalabilidade

**Throughput**
- Limitado pela API externa de embeddings
- Tipicamente: 5-30 segundos por job
- Varia com tamanho do lote e lat√™ncia da API

**Escalabilidade Horizontal**
- Workers podem ser escalados independentemente
- Kafka distribui carga automaticamente
- M√∫ltiplas inst√¢ncias do mesmo consumer group

**Otimiza√ß√µes Futuras**
- Cache de embeddings para descri√ß√µes similares
- Batching de m√∫ltiplos jobs para API de embeddings
- WebSockets para notifica√ß√µes ao inv√©s de polling

---

## Tecnologias Utilizadas

### Backend e API
- **Flask 3.1.0**: Framework web minimalista e flex√≠vel
- **Flask-OpenAPI3**: Documenta√ß√£o autom√°tica OpenAPI 3.0
- **Flask-CORS**: Gerenciamento de CORS para comunica√ß√£o com frontend
- **Pydantic 2.10.6**: Valida√ß√£o de dados e serializa√ß√£o
- **Kafka/quixstreams 2.4**: Message broker para processamento ass√≠ncrono

### Banco de Dados
- **SQLAlchemy 2.0.39**: ORM para Python
- **SQLite**: Banco de dados relacional leve

### Machine Learning
- **scikit-learn 1.7.0**: Modelos de classifica√ß√£o e pipelines
- **pandas 2.3.0**: Manipula√ß√£o de dados
- **numpy 2.2.6**: Computa√ß√£o num√©rica
- **google-genai 1.53.0**: Cliente para Google Gemini API

### Seguran√ßa
- **cryptography 45.0.5**: Criptografia de dados sens√≠veis
- **python-dotenv 1.2.1**: Gerenciamento de vari√°veis de ambiente

### Desenvolvimento e Testes
- **pytest 8.4.0**: Framework de testes
- **jupyter**: Notebooks para experimenta√ß√£o
- **joblib**: Serializa√ß√£o de modelos

---

## Configura√ß√£o do Ambiente

### Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto baseado no arquivo `env.example`:

```bash
# Chave da API do Google Gemini (OBRIGAT√ìRIO para classifica√ß√£o)
GEMINI_API_KEY=sua_chave_api_aqui

# Modelo de embedding (OPCIONAL - padr√£o: models/gemini-embedding-001)
GEMINI_MODEL=models/gemini-embedding-001

# Caminho customizado para modelos ML (OPCIONAL)
# Se n√£o especificado, usa: machine_learning/transactions_classification/models
MODEL_PATH=

# Chave de criptografia para datasets (OBRIGAT√ìRIO para funcionalidades de seguran√ßa)
ENC_KEY=sua_chave_de_criptografia_aqui

# Kafka Configuration (para processamento ass√≠ncrono)
KAFKA_BROKER_ADDRESS=localhost:9092
BATCH_JOBS_TOPIC=batch-jobs
EMBEDDINGS_RESULTS_TOPIC=embeddings-results
EMBEDDINGS_CONSUMER_GROUP=embeddings_worker
CLASSIFICATION_CONSUMER_GROUP=classification_worker

# External Embedding API
EMBEDDING_API_URL=http://localhost:8000
```

**A API Key do Google Gemini foi enviada na descri√ß√£o do MVP, pela plataforma.**

---

## Como Instalar as Depend√™ncias

Siga os passos abaixo para configurar o ambiente de desenvolvimento:

### 1. **Clone o Reposit√≥rio**
```bash
git clone https://github.com/GuilhermePFM/mvp-api
cd mvp-api
```

### 2. **Crie um Ambiente Virtual**
√â recomendado usar um ambiente virtual para isolar as depend√™ncias:

```bash
python -m venv venv
```

### 3. **Ative o Ambiente Virtual**

**No Windows:**
```bash
venv\Scripts\activate
```

**No macOS/Linux:**
```bash
source venv/bin/activate
```

### 4. **Instale as Depend√™ncias**
```bash
pip install -r requirements.txt
```

Este comando instala todas as bibliotecas necess√°rias listadas no arquivo `requirements.txt`, incluindo Flask, SQLAlchemy, scikit-learn, e outras depend√™ncias essenciais.

### 5. **Configure as Vari√°veis de Ambiente**
```bash
# Copie o arquivo de exemplo
cp env.example .env

# Edite o arquivo .env e adicione suas chaves
# Certifique-se de adicionar sua GEMINI_API_KEY
```

---

## Como Executar o C√≥digo

### 1. **Inicie a Aplica√ß√£o**
```bash
python -m flask run --no-debugger --no-reload
```

Ou simplesmente:
```bash
python app.py
```

### 2. **Acesse a API**
A API estar√° dispon√≠vel em: [http://127.0.0.1:5000](http://127.0.0.1:5000)

### 3. **Acesse a Documenta√ß√£o Interativa**
A interface Swagger estar√° dispon√≠vel em:
- **Swagger UI**: [http://127.0.0.1:5000/openapi/swagger](http://127.0.0.1:5000/openapi/swagger)
- **ReDoc**: [http://127.0.0.1:5000/openapi/redoc](http://127.0.0.1:5000/openapi/redoc)

---

## üê≥ Implanta√ß√£o com Docker

Docker permite executar a aplica√ß√£o em um ambiente isolado e consistente, facilitando o desenvolvimento e a implanta√ß√£o em produ√ß√£o.

### Pr√©-requisitos

- **Docker**: [Instalar Docker](https://docs.docker.com/get-docker/)
- **Docker Compose**: [Instalar Docker Compose](https://docs.docker.com/compose/install/)

### Desenvolvimento Local com Docker

Para executar a aplica√ß√£o em ambiente de desenvolvimento usando Docker:

#### 1. **Configure as Vari√°veis de Ambiente**
```bash
# Copie o arquivo de exemplo
cp env.example .env

# Edite o arquivo .env e adicione suas chaves
# IMPORTANTE: Adicione sua GEMINI_API_KEY
```

#### 2. **Build e Inicie os Containers**
```bash
docker-compose up -d
```

Este comando ir√°:
- Construir a imagem Docker da aplica√ß√£o
- Iniciar o container em segundo plano (`-d` = detached)
- Montar volumes para persist√™ncia de dados

#### 3. **Verifique os Logs**
```bash
# Ver logs em tempo real
docker-compose logs -f

# Ver logs apenas do servi√ßo app
docker-compose logs -f app
```

#### 4. **Acesse a Aplica√ß√£o**
- **API**: [http://localhost:5000](http://localhost:5000)
- **Swagger UI**: [http://localhost:5000/openapi/swagger](http://localhost:5000/openapi/swagger)

#### 5. **Parar a Aplica√ß√£o**
```bash
# Parar containers (mant√©m volumes)
docker-compose down

# Parar e remover volumes (‚ö†Ô∏è apaga dados)
docker-compose down -v
```

### Volumes e Persist√™ncia de Dados

O Docker est√° configurado com os seguintes volumes para garantir persist√™ncia:

| Volume | Diret√≥rio Host | Diret√≥rio Container | Descri√ß√£o |
|--------|---------------|---------------------|-----------|
| **Database** | `./database/` | `/app/database/` | Banco de dados SQLite |
| **Logs** | `./log/` | `/app/log/` | Logs da aplica√ß√£o e Gunicorn |
| **Env** | `./.env` | `/app/.env` | Vari√°veis de ambiente (read-only) |

**Importante**: Os dados persistem mesmo ap√≥s `docker-compose down`. Use `docker-compose down -v` apenas se quiser apagar todos os dados.

### Portas Expostas

| Servi√ßo | Porta Host | Porta Container | Descri√ß√£o |
|---------|-----------|-----------------|-----------|
| **API** | 5000 | 5000 | Flask/Gunicorn API |


## Endpoints da API

### Documenta√ß√£o Interativa

A API oferece documenta√ß√£o interativa completa atrav√©s do **Swagger UI**, onde voc√™ pode:
- Visualizar todos os endpoints dispon√≠veis
- Testar requisi√ß√µes diretamente no navegador
- Ver exemplos de payloads e respostas
- Entender os schemas de dados

**Acesse**: [http://127.0.0.1:5000/openapi/swagger](http://127.0.0.1:5000/openapi/swagger)

### Principais Endpoints

#### ü§ñ **Classifica√ß√£o de Transa√ß√µes**

**`POST /batchclassifier`** (S√≠ncrono)
- Classifica automaticamente um lote de transa√ß√µes usando Machine Learning
- **Payload**: Lista de transa√ß√µes com `value`, `date`, `description`
- **Resposta**: Lista de transa√ß√µes com campo `classification` adicionado
- **Uso**: Para lotes pequenos com resposta imediata

```json
{
  "transactions": [
    {
      "value": 45.90,
      "date": "2024-01-15",
      "description": "Uber viagem centro",
      "user": 1,
      "classification": null
    }
  ]
}
```

**`POST /api/batch-classify-async`** (Ass√≠ncrono - Novo!)
- Submete job de classifica√ß√£o para processamento ass√≠ncrono
- **Payload**: Lista de transa√ß√µes (igual ao endpoint s√≠ncrono)
- **Resposta**: `202 Accepted` com `jobId` para polling
- **Uso**: Para lotes grandes ou quando o tempo de resposta √© alto

```json
// Request
{
  "transactions": [
    {
      "date": "2024-01-15T00:00:00",
      "description": "Grocery shopping",
      "value": 150.50,
      "user": "John Doe",
      "classification": null
    }
  ]
}

// Response (202 Accepted)
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000"
}
```

**`GET /api/batch-jobs/{jobId}`**
- Consulta status de job ass√≠ncrono
- **Resposta**: Status atual + resultados (se completo)
- **Estados**: `pending`, `processing`, `completed`, `failed`

```json
// Status: processing
{
  "status": "processing"
}

// Status: completed
{
  "status": "completed",
  "transactions": [
    {
      "date": "2024-01-15T00:00:00",
      "description": "Grocery shopping",
      "value": 150.50,
      "user": "John Doe",
      "classification": "Food & Groceries"
    }
  ]
}

// Status: failed
{
  "status": "failed",
  "message": "External API unavailable"
}
```

#### üë§ **Usu√°rios**

- **`POST /user`**: Adiciona novo usu√°rio
- **`GET /user?email=usuario@example.com`**: Busca usu√°rio por email
- **`GET /users`**: Lista todos os usu√°rios
- **`DELETE /user?email=usuario@example.com`**: Remove usu√°rio

#### üí∞ **Transa√ß√µes**

- **`POST /transaction`**: Adiciona nova transa√ß√£o individual
- **`POST /transactions`**: Adiciona m√∫ltiplas transa√ß√µes em lote
- **`GET /transactions`**: Lista todas as transa√ß√µes
- **`DELETE /transaction?id=1`**: Remove transa√ß√£o espec√≠fica

#### üè∑Ô∏è **Categorias**

- **`POST /category`**: Cria nova categoria
- **`GET /categories`**: Lista todas as categorias
- **`DELETE /category?id=1`**: Remove categoria

#### üìã **Tipos de Transa√ß√£o**

- **`POST /type`**: Cria novo tipo (Receita/Despesa)
- **`GET /types`**: Lista todos os tipos
- **`DELETE /type?id=1`**: Remove tipo

---

## Testes

O projeto inclui uma su√≠te de testes automatizados para garantir a qualidade e confiabilidade do c√≥digo.

### Executar Todos os Testes
```bash
pytest
```

### Executar Testes Espec√≠ficos
```bash
# Testes do modelo de Machine Learning
pytest tests/test_transactions_ml_model.py

# Testes de integra√ß√£o
pytest tests/test_integration_transactions_ml_model.py

# Testes de criptografia
pytest tests/test_dataset_encryption.py
```

### Executar com Cobertura
```bash
pytest --cov=. --cov-report=html
```

### Estrutura de Testes

- **`test_transactions_ml_model.py`**: Testa a performance e precis√£o do modelo de classifica√ß√£o
- **`test_integration_transactions_ml_model.py`**: Testa a integra√ß√£o completa do pipeline de ML
- **`test_dataset_encryption.py`**: Valida os mecanismos de criptografia de dados
- **`test_async_batch_classifier.py`**: Testa endpoints ass√≠ncronos, workers Kafka e retry logic
- **`classification_fixtures/`**: Fixtures com dados de teste e resultados esperados

### Testar Endpoints Ass√≠ncronos

```bash
# Testes de endpoints ass√≠ncronos
pytest tests/test_async_batch_classifier.py -v

# Testar apenas retry logic
pytest tests/test_async_batch_classifier.py::TestRetryLogic -v

# Testar job cleanup
pytest tests/test_async_batch_classifier.py::TestJobCleanup -v
```

---

## Seguran√ßa

O sistema implementa m√∫ltiplas camadas de seguran√ßa para proteger dados sens√≠veis:

### üîê Criptografia de Dados
- **Criptografia Sim√©trica**: Datasets com informa√ß√µes financeiras s√£o criptografados antes de serem armazenados
- **Chave de Criptografia**: Gerenciada via vari√°vel de ambiente `ENC_KEY`
- **M√≥dulo**: `security/dataset.py`

### üõ°Ô∏è Prote√ß√£o de Informa√ß√µes Pessoais
- Dados de usu√°rios e transa√ß√µes s√£o tratados com pol√≠ticas de seguran√ßa rigorosas
- Sistema de salting para prote√ß√£o adicional

---

**Desenvolvido como parte do MVP para P√≥s-Gradua√ß√£o em Engenharia de Software - PUC Rio**